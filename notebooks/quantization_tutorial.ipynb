{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # <a id='toc1_'></a>[Table of Contents](#toc0_)\n",
    "* [What is Quantization and why we use it ?](#chapter1)\n",
    "    * [Section 1.1](#section_1_1)\n",
    "* [Chapter 2](#chapter2)\n",
    "    * [Section 2.1](#section_2_1)\n",
    "        * [Sub Section 2.1.1](#sub_section_2_1_1)\n",
    "        * [Sub Section 2.1.2](#sub_section_2_1_2)\n",
    "* [Chapter 3](#chapter3)\n",
    "    * [Section 3.1](#section_3_1)\n",
    "        * [Sub Section 3.1.1](#sub_section_3_1_1)\n",
    "        * [Sub Section 3.1.2](#sub_section_3_1_2)\n",
    "    * [Section 3.2](#section_3_2)\n",
    "        * [Sub Section 3.2.1](#sub_section_3_2_1) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "\n",
    "Quantization is a technique for performing computations and storing tensors at lower bitwidths than floating point precision. A quantized model executes some or all of the operations on tensors with reduced precision rather than full precision (floating point) values <sup>[1](#1)</sup> . This allows for a more compact model representation and the use of high performance vectorized operations on many hardware platforms. PyTorch supports INT8 quantization compared to typical FP32 models allowing for a 4x reduction in the model size and a 4x reduction in memory bandwidth requirements. Hardware support for INT8 computations is typically 2 to 4 times faster compared to FP32 compute. Quantization is primarily a technique to speed up inference and only the forward pass is supported for quantized operators [1]. PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization <sup>[1](#1)</sup> . \n",
    "\n",
    "\n",
    "Quantization in TensorFlow refers to the process of reducing the precision of the weights and/or activations of a neural network model. The goal is to represent the model with lower bit precision (e.g., 8-bit integers) rather than the standard 32-bit floating-point numbers. This reduction in precision can lead to more efficient model deployment, especially on hardware with limited computational resources, such as edge devices and mobile devices <sup>[2](#1)</sup>.\n",
    "\n",
    "[1]<a class=\"anchor\" id=\"1\"></a>: https://pytorch.org/docs/stable/quantization.html\n",
    "\n",
    "\n",
    "[2]<a class=\"anchor\" id=\"2\"></a>: https://www.tensorflow.org/api_docs/python/tf/quantization/quantize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding and Applying Quantization\n",
    "\n",
    "Quantization is a method that can allow models to run faster and use less memory. By converting 32-bit floating-point numbers (the `float32` data type) into lower-precision formats, like 8-bit integers (the `int8` data type), we can reduce the computational requirements of our models. Let's start with the basics and gradually move towards quantizing complex models like CNNs.\n",
    "\n",
    "### Learning Objectives\n",
    "1. Explore how to quantize a single variable and a function in pytorch\n",
    "1. Apply quantization to a neural network\n",
    "1. Compare the size and performance of quantized convolutional neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 - Quantization\n",
    "\n",
    "We'll illustrate both 4-bit and 8-bit quantization. As for the neural network part, we'll create a simple model and show how to quantize and dequantize its weights. I'll present the code you would use to do it.\n",
    "\n",
    "\n",
    "\n",
    "### Quantization of a Single Value\n",
    "Quantization is the process of constraining an input from a large set to output in a smaller set. In the context of deep learning, it's used to reduce the precision of the weights and activations of the neural network models. This can help to reduce the memory footprint and computational intensity of models. Here, we'll start by quantizing a single floating point number.\n",
    "\n",
    "We'll define two functions: one to quantize a value and another to unquantize it. The quantize function will take a floating point number and a number of bits, and will output an integer representation of the input number. The unquantize function will take the integer and the number of bits, and will output the floating point number.\n",
    "\n",
    "The range of input values for the quantize function is between -1 and 1. The range of output values for the unquantize function is also between -1 and 1. The number of bits determines the precision of the quantization. More bits means higher precision, but more memory usage. For this demonstration, we'll use 4 and 8 bits.\n",
    "\n",
    "## Quantization and Dequantization of Floating Point Numbers\n",
    "\n",
    "Quantization is a technique used for performing computations and storing tensors at lower bitwidths than floating-point precision. The process involves converting floating-point numbers to a lower bit representation, and later dequantizing them back to their original precision. Here are the formulas for quantization and dequantization:\n",
    "\n",
    "### Quantization Formula\n",
    "\n",
    "The quantization formula typically involves rounding the floating-point number to the nearest representable value within the reduced bitwidth. Let `x` be the original floating-point number, `q` be the quantized value, and `S` be the scaling factor:\n",
    "\n",
    "\\[ q = \\text{round}(x \\times S) \\]\n",
    "\n",
    "### Dequantization Formula\n",
    "\n",
    "The dequantization process involves converting the quantized value back to the original floating-point precision. Let `q` be the quantized value, `x'` be the dequantized value, and `S` be the scaling factor:\n",
    "\n",
    "\\[ x' = \\frac{q}{S} \\]\n",
    "\n",
    "In these formulas, `S` is often chosen to be a power of 2 to simplify the quantization process, and it is used to scale the floating-point numbers before rounding during quantization and after scaling during dequantization.\n",
    "\n",
    "It's important to note that the choice of scaling factor and bitwidth greatly influences the precision and range of representable values after quantization and dequantization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Quantization object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantization:\n",
    "    \"\"\"\n",
    "    Quantization class for representing and manipulating quantized values.\n",
    "\n",
    "    Attributes:\n",
    "        value (float): The original floating-point value to be quantized.\n",
    "        bits (int): The number of bits used for quantization.\n",
    "        quantized_value (int): The quantized value resulting from the quantization process.\n",
    "\n",
    "    Methods:\n",
    "        __init__(self, value: float, bits: int):\n",
    "            Initializes a Quantization object with the given original value and bitwidth.\n",
    "\n",
    "        quantize(self) -> int:\n",
    "            Quantizes the original floating-point value and stores the result in 'quantized_value'.\n",
    "            Returns the quantized value as an integer.\n",
    "\n",
    "        unquantize(self) -> float:\n",
    "            Dequantizes the quantized value and returns the original floating-point value.\n",
    "            Raises a ValueError if 'quantize' has not been called before attempting to unquantize.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value: float, bits: int):\n",
    "        \"\"\"\n",
    "        Initializes a Quantization object.\n",
    "\n",
    "        Parameters:\n",
    "            value (float): The original floating-point value to be quantized.\n",
    "            bits (int): The number of bits used for quantization.\n",
    "        \"\"\"\n",
    "        self.value = value\n",
    "        self.bits = bits\n",
    "        self.quantized_value = None\n",
    "\n",
    "    def quantize(self) -> int:\n",
    "        \"\"\"\n",
    "        Quantizes the original floating-point value and stores the result in 'quantized_value'.\n",
    "        \n",
    "        Returns:\n",
    "            int: The quantized value as an integer.\n",
    "        \"\"\"\n",
    "        quantized_value = np.round(self.value * (2**(self.bits - 1) - 1))\n",
    "        self.quantized_value = quantized_value\n",
    "        return int(quantized_value)\n",
    "\n",
    "    def unquantize(self) -> float:\n",
    "        \"\"\"\n",
    "        Dequantizes the quantized value and returns the original floating-point value.\n",
    "\n",
    "        Returns:\n",
    "            float: The dequantized value.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If 'quantize' has not been called before attempting to unquantize.\n",
    "        \"\"\"\n",
    "        if self.quantized_value is None:\n",
    "            raise ValueError(\"Please quantize the value first.\")\n",
    "        unquantized_value = self.quantized_value / (2**(self.bits -1) - 1)\n",
    "        self.unquantized_value = unquantized_value\n",
    "        return float(unquantized_value)\n",
    "    \n",
    "    def calculate_error(self) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the absolute error between the original value and the dequantized value.\n",
    "\n",
    "        Returns:\n",
    "            float: The absolute error.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If 'quantize' has not been called before attempting to calculate the error.\n",
    "        \"\"\"\n",
    "        if self.quantized_value is None:\n",
    "            raise ValueError(\"Please quantize the value first.\")\n",
    "        return abs(self.value - self.unquantize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the quantize and unquantize functions with 4 and 8 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Value: 3.141592653589793\n",
      "----\n",
      "4-bit Quantization:22\n",
      "4-bit Unquantization: 3.142857142857143\n",
      "----\n",
      "8-bit Quantization:399\n",
      "8-bit Unquantization: 3.141732283464567\n"
     ]
    }
   ],
   "source": [
    "value = 3.141592653589793\n",
    "\n",
    "value_4bit             = Quantization(value, bits=4)\n",
    "quantized_value_4bit   = value_4bit.quantize()\n",
    "unquantized_value_4bit = value_4bit.unquantize()\n",
    "\n",
    "value_8bit             = Quantization(value, bits=8)\n",
    "quantized_value_8bit   = value_8bit.quantize()\n",
    "unquantized_value_8bit = value_8bit.unquantize()\n",
    "\n",
    "print(f\"Original Value: {value}\\n----\\n4-bit Quantization:{quantized_value_4bit}\\n4-bit Unquantization: {unquantized_value_4bit}\\n----\\n8-bit Quantization:{quantized_value_8bit}\\n8-bit Unquantization: {unquantized_value_8bit}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
